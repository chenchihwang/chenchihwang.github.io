---
published: 2025-10-18
---

mitigating the risk of exticntion from ai should bea gloabl priority alongside other socieal scale risks such as pandemics nad nuclear war **insofar as** 
1) ai has the potential x-risk level extinction factor as pandemics / nuclear war
2) it is feasible to reach such a level of ai
3) work to reduce the risk is effective. 

could grab aditya, eric, meru tgt for next apart hackathon 

going also to do a big notes change. gonna switch up the connotation and be high agency. will explicitly map this out in my journal. i can no longer be mid because:
1) im a bf now and i cant be mid
2) this gcp program is too transformative in telling me what i need to do. aint no time for trolling.